{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Dimensionality Reduction Analysis\n\nThis notebook explores various dimensionality reduction techniques on our preprocessed datasets:\n\n1. **Principal Component Analysis (PCA)** - Linear dimensionality reduction\n2. **t-SNE** - Non-linear manifold learning for visualization  \n3. **UMAP** - Uniform Manifold Approximation and Projection\n4. **Linear Discriminant Analysis (LDA)** - Supervised dimensionality reduction\n\n## Objectives:\n- Visualize high-dimensional data in 2D/3D space\n- Understand data structure and class separability\n- Identify optimal dimensionality for downstream tasks\n- Compare linear vs non-linear reduction methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\n\n# Dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport umap\nimport optuna\nfrom sklearn.metrics import silhouette_score\n\n# Custom modules\nfrom dimensionality.reduction import DimensionalityReducer\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Load Preprocessed Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load preprocessed data\nprocessed_dir = \"../data/processed\"\n\n# BBB Dataset\nX_bbb_train = np.load(f\"{processed_dir}/X_bbb_train.npy\")\nX_bbb_test = np.load(f\"{processed_dir}/X_bbb_test.npy\")\ny_bbb_train = np.load(f\"{processed_dir}/y_bbb_train.npy\")\ny_bbb_test = np.load(f\"{processed_dir}/y_bbb_test.npy\")\n\n# Breast Cancer Dataset\nX_bc_train = np.load(f\"{processed_dir}/X_bc_train.npy\")\nX_bc_test = np.load(f\"{processed_dir}/X_bc_test.npy\")\ny_bc_train = np.load(f\"{processed_dir}/y_bc_train.npy\")\ny_bc_test = np.load(f\"{processed_dir}/y_bc_test.npy\")\n\nprint(\"Loaded preprocessed datasets:\")\nprint(f\"BBB - Train: {X_bbb_train.shape}, Test: {X_bbb_test.shape}\")\nprint(f\"BC  - Train: {X_bc_train.shape}, Test: {X_bc_test.shape}\")\n\n# Check class distributions\nprint(f\"\\nClass distributions:\")\nprint(f\"BBB train: {np.bincount(y_bbb_train)}\")\nprint(f\"BC train:  {np.bincount(y_bc_train)}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Helper Functions for Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def setup_2d_subplot(ax, data, labels, title):\n    \"\"\"\n    Setup a 2D subplot with the given data and labels, minimal axes style,\n    with arrow-like x/y axes, no spines, no grid, no tick labels.\n    \"\"\"\n    # colormap for binary classification: red/green\n    rg_cmap = ListedColormap([\"red\", \"green\"])\n    \n    ax.scatter(data[:, 0], data[:, 1],\n               c=labels, cmap=rg_cmap, s=50, edgecolor='k', alpha=0.8)\n    ax.set_title(title, fontsize=14)\n    \n    # Remove all spines\n    for spine in ax.spines.values():\n        spine.set_visible(False)\n    \n    # Remove ticks\n    ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    \n    # Remove grid\n    ax.grid(False)\n    \n    x_min, x_max = ax.get_xlim()\n    y_min, y_max = ax.get_ylim()\n    x_half = (x_max - x_min) / 2.0\n    y_half = (y_max - y_min) / 2.0\n    \n    # Add arrowed axes\n    ax.annotate(\"\",\n                xy=(x_min + x_half, y_min),\n                xytext=(x_min, y_min),\n                arrowprops=dict(arrowstyle=\"->\", lw=3, color='k'))\n    ax.annotate(\"\",\n                xy=(x_min, y_min + y_half),\n                xytext=(x_min, y_min),\n                arrowprops=dict(arrowstyle=\"->\", lw=3, color='k'))\n    \n    # Offsets for axis labels\n    x_offset = 0.03 * (y_max - y_min)\n    y_offset = 0.03 * (x_max - x_min)\n    \n    # Dynamically choose prefix based on the title\n    title_upper = title.upper()\n    if \"PCA\" in title_upper:\n        comp_name = \"PC\"\n    elif \"UMAP\" in title_upper:\n        comp_name = \"UMAP\"\n    elif \"T-SNE\" in title_upper or \"TSNE\" in title_upper:\n        comp_name = \"t-SNE\"\n    else:\n        comp_name = \"Component\"\n    \n    ax.text(x_min + x_half, y_min - x_offset, f\"{comp_name}1\",\n            fontsize=12, fontweight='bold', ha=\"center\", va=\"top\")\n    ax.text(x_min - y_offset, y_min + y_half, f\"{comp_name}2\",\n            fontsize=12, fontweight='bold', ha=\"right\", va=\"center\", rotation=90)\n\ndef plot_cumulative_variance(pca, dataset_name, variance_threshold=0.90):\n    \"\"\"Plot cumulative explained variance for PCA\"\"\"\n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n             'bo-', linewidth=2, markersize=6)\n    plt.axhline(y=variance_threshold, color='r', linestyle='--', \n                label=f'{variance_threshold*100:.0f}% Variance Threshold')\n    \n    # Find number of components for threshold\n    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n    plt.axvline(x=n_components, color='g', linestyle=':', \n                label=f'{n_components} Components')\n    \n    plt.xlabel('Number of Principal Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title(f'{dataset_name} - PCA Cumulative Explained Variance')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return n_components\n\nprint(\"Helper functions defined!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Principal Component Analysis (PCA)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Perform PCA analysis for BBB dataset\nprint(\"=== BBB Dataset PCA Analysis ===\")\n\n# Full PCA to analyze variance\npca_bbb_full = PCA()\npca_bbb_full.fit(X_bbb_train)\n\n# Plot cumulative variance explained\nn_components_bbb = plot_cumulative_variance(pca_bbb_full, \"BBB Dataset\", variance_threshold=0.90)\n\nprint(f\"Components needed for 90% variance: {n_components_bbb}\")\nprint(f\"Explained variance by first 10 components:\")\nfor i in range(min(10, len(pca_bbb_full.explained_variance_ratio_))):\n    print(f\"  PC{i+1}: {pca_bbb_full.explained_variance_ratio_[i]:.4f}\")\n\n# 2D PCA visualization\npca_bbb_2d = PCA(n_components=2)\nX_bbb_pca_2d = pca_bbb_2d.fit_transform(X_bbb_train)\n\nprint(f\"\\n2D PCA - Explained variance ratio: {pca_bbb_2d.explained_variance_ratio_}\")\nprint(f\"Total explained variance: {sum(pca_bbb_2d.explained_variance_ratio_):.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Breast Cancer PCA analysis\nprint(\"\\n=== Breast Cancer Dataset PCA Analysis ===\")\n\n# Full PCA\npca_bc_full = PCA()\npca_bc_full.fit(X_bc_train)\n\n# Plot cumulative variance\nn_components_bc = plot_cumulative_variance(pca_bc_full, \"Breast Cancer Dataset\", variance_threshold=0.90)\n\nprint(f\"Components needed for 90% variance: {n_components_bc}\")\nprint(f\"Explained variance by first 10 components:\")\nfor i in range(min(10, len(pca_bc_full.explained_variance_ratio_))):\n    print(f\"  PC{i+1}: {pca_bc_full.explained_variance_ratio_[i]:.4f}\")\n\n# 2D PCA visualization\npca_bc_2d = PCA(n_components=2)\nX_bc_pca_2d = pca_bc_2d.fit_transform(X_bc_train)\n\nprint(f\"\\n2D PCA - Explained variance ratio: {pca_bc_2d.explained_variance_ratio_}\")\nprint(f\"Total explained variance: {sum(pca_bc_2d.explained_variance_ratio_):.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot PCA visualizations side by side\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsetup_2d_subplot(axes[0], X_bbb_pca_2d, y_bbb_train, \"BBB Dataset PCA\")\nsetup_2d_subplot(axes[1], X_bc_pca_2d, y_bc_train, \"Breast Cancer Dataset PCA\")\n\nplt.suptitle(\"PCA Visualization Comparison\", fontsize=16)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. t-SNE (t-Distributed Stochastic Neighbor Embedding)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply t-SNE to both datasets\nprint(\"Applying t-SNE...\")\n\n# BBB Dataset t-SNE\nprint(\"Computing t-SNE for BBB dataset...\")\ntsne_bbb = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\nX_bbb_tsne = tsne_bbb.fit_transform(X_bbb_train)\n\n# Breast Cancer Dataset t-SNE\nprint(\"Computing t-SNE for Breast Cancer dataset...\")\ntsne_bc = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\nX_bc_tsne = tsne_bc.fit_transform(X_bc_train)\n\nprint(\"t-SNE computation completed!\")\n\n# Plot t-SNE visualizations\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsetup_2d_subplot(axes[0], X_bbb_tsne, y_bbb_train, \"BBB Dataset t-SNE\")\nsetup_2d_subplot(axes[1], X_bc_tsne, y_bc_train, \"Breast Cancer Dataset t-SNE\")\n\nplt.suptitle(\"t-SNE Visualization Comparison\", fontsize=16)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. UMAP (Uniform Manifold Approximation and Projection)\n\nUMAP is a modern dimensionality reduction technique that often provides better global structure preservation than t-SNE.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply UMAP to both datasets\nprint(\"Applying UMAP...\")\n\n# UMAP configuration based on optimization results\numap_config = {\n    'n_neighbors': 28,\n    'min_dist': 0.04,\n    'n_components': 2,\n    'target_metric': 'categorical',\n    'target_weight': 0.5,\n    'random_state': 42\n}\n\n# BBB Dataset UMAP (supervised)\nprint(\"Computing supervised UMAP for BBB dataset...\")\numap_bbb = umap.UMAP(**umap_config)\nX_bbb_umap = umap_bbb.fit_transform(X_bbb_train, y_bbb_train)\n\n# Breast Cancer Dataset UMAP (supervised)\nprint(\"Computing supervised UMAP for Breast Cancer dataset...\")\numap_bc = umap.UMAP(**umap_config)\nX_bc_umap = umap_bc.fit_transform(X_bc_train, y_bc_train)\n\nprint(\"UMAP computation completed!\")\n\n# Plot UMAP visualizations\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsetup_2d_subplot(axes[0], X_bbb_umap, y_bbb_train, \"BBB Dataset UMAP\")\nsetup_2d_subplot(axes[1], X_bc_umap, y_bc_train, \"Breast Cancer Dataset UMAP\")\n\nplt.suptitle(\"UMAP Visualization Comparison\", fontsize=16)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Linear Discriminant Analysis (LDA)\n\nLDA is a supervised dimensionality reduction technique that maximizes class separability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply LDA to both datasets\nprint(\"Applying LDA...\")\n\n# For binary classification, LDA extracts at most 1 component\nn_components = 1\n\n# BBB Dataset LDA\nlda_bbb = LinearDiscriminantAnalysis(n_components=n_components)\nX_bbb_lda = lda_bbb.fit_transform(X_bbb_train, y_bbb_train)\n\n# Breast Cancer Dataset LDA\nlda_bc = LinearDiscriminantAnalysis(n_components=n_components)\nX_bc_lda = lda_bc.fit_transform(X_bc_train, y_bc_train)\n\nprint(f\"LDA completed! Extracted {n_components} discriminant component(s)\")\n\n# Plot LDA distributions\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# BBB Dataset LDA distribution\nfor label in np.unique(y_bbb_train):\n    subset = X_bbb_lda[y_bbb_train == label].ravel()\n    axes[0].hist(subset, alpha=0.7, label=f\"Class {label}\", bins=30, density=True)\naxes[0].set_xlabel(\"LDA Component 1\")\naxes[0].set_ylabel(\"Density\")\naxes[0].set_title(\"BBB Dataset LDA\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Breast Cancer Dataset LDA distribution\nfor label in np.unique(y_bc_train):\n    subset = X_bc_lda[y_bc_train == label].ravel()\n    axes[1].hist(subset, alpha=0.7, label=f\"Class {label}\", bins=30, density=True)\naxes[1].set_xlabel(\"LDA Component 1\")\naxes[1].set_ylabel(\"Density\")\naxes[1].set_title(\"Breast Cancer Dataset LDA\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle(\"LDA Class Separation Analysis\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Print LDA statistics\nprint(f\"\\nLDA Analysis:\")\nprint(f\"BBB Dataset - Class separability (between-class variance):\")\nprint(f\"  Explained variance ratio: {lda_bbb.explained_variance_ratio_}\")\nprint(f\"BC Dataset - Class separability:\")\nprint(f\"  Explained variance ratio: {lda_bc.explained_variance_ratio_}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Comprehensive Comparison\n\nLet's create a comprehensive visualization comparing all dimensionality reduction techniques:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive comparison plot\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# BBB Dataset (top row)\nsetup_2d_subplot(axes[0, 0], X_bbb_pca_2d, y_bbb_train, \"BBB - PCA\")\nsetup_2d_subplot(axes[0, 1], X_bbb_tsne, y_bbb_train, \"BBB - t-SNE\")\nsetup_2d_subplot(axes[0, 2], X_bbb_umap, y_bbb_train, \"BBB - UMAP\")\n\n# Breast Cancer Dataset (bottom row)\nsetup_2d_subplot(axes[1, 0], X_bc_pca_2d, y_bc_train, \"BC - PCA\")\nsetup_2d_subplot(axes[1, 1], X_bc_tsne, y_bc_train, \"BC - t-SNE\")\nsetup_2d_subplot(axes[1, 2], X_bc_umap, y_bc_train, \"BC - UMAP\")\n\nplt.suptitle(\"Dimensionality Reduction Techniques Comparison\", fontsize=20)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Apply Optimal PCA for BBB Dataset\n\nBased on the variance analysis, we'll apply PCA to reduce the BBB dataset to the optimal number of components:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply optimal PCA to BBB dataset for active learning\nprint(f\"Applying PCA with {n_components_bbb} components to BBB dataset...\")\n\n# Use the optimal number of components identified earlier\npca_bbb_optimal = PCA(n_components=n_components_bbb)\nX_bbb_train_pca = pca_bbb_optimal.fit_transform(X_bbb_train)\nX_bbb_test_pca = pca_bbb_optimal.transform(X_bbb_test)\n\nprint(f\"PCA transformation completed:\")\nprint(f\"  Original BBB train shape: {X_bbb_train.shape}\")\nprint(f\"  PCA BBB train shape: {X_bbb_train_pca.shape}\")\nprint(f\"  Original BBB test shape: {X_bbb_test.shape}\")\nprint(f\"  PCA BBB test shape: {X_bbb_test_pca.shape}\")\nprint(f\"  Explained variance: {sum(pca_bbb_optimal.explained_variance_ratio_):.4f}\")\n\n# Save the PCA-transformed data for active learning experiments\nnp.save(f\"{processed_dir}/X_bbb_train_pca.npy\", X_bbb_train_pca)\nnp.save(f\"{processed_dir}/X_bbb_test_pca.npy\", X_bbb_test_pca)\n\nprint(f\"PCA-transformed BBB data saved to {processed_dir}/\")\n\n# For comparison: Breast Cancer dataset doesn't need PCA (only 30 features)\nprint(f\"\\nBreast Cancer dataset: {X_bc_train.shape[1]} features - no PCA needed\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\n**Dimensionality Reduction Analysis Complete!**\n\n### Key Findings:\n\n#### BBB Dataset:\n- **Original dimensions**: 500+ molecular features\n- **PCA optimal**: 90% variance retained with ~100-150 components\n- **Visualization**: Class separation moderate in 2D projections\n- **Recommendation**: Use PCA-reduced features for active learning\n\n#### Breast Cancer Dataset:\n- **Original dimensions**: 30 clinical features\n- **PCA variance**: First 2 components explain ~63% variance\n- **Visualization**: Strong class separation visible in all methods\n- **Recommendation**: Use original features (no dimensionality reduction needed)\n\n### Method Comparison:\n1. **PCA**: Linear, fast, preserves global structure, good for feature reduction\n2. **t-SNE**: Non-linear, good local structure, can create artificial clusters\n3. **UMAP**: Non-linear, faster than t-SNE, better global structure preservation\n4. **LDA**: Supervised, maximizes class separation, limited to n_classes-1 dimensions\n\n### For Active Learning:\n- BBB dataset: Use PCA-reduced features for efficiency while retaining 90% variance\n- Breast Cancer: Use original 30 features directly\n- Both datasets show sufficient class separability for active learning experiments\n\n**Next Step**: Active learning experiments using optimally processed data",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}